
Epoch 0 training loss: 5473008374251520.0
Epoch 15 training loss: 2531521235779584.0
Epoch 30 training loss: 1097908665450496.0
Epoch 45 training loss: 463723508531200.0
Epoch 60 training loss: 183946897784832.0
Epoch 75 training loss: 65916515647488.0
Epoch 90 training loss: 20886961782784.0
Epoch 105 training loss: 5793443479552.0
Epoch 120 training loss: 1399584391168.0
Epoch 135 training loss: 293305679872.0
Epoch 150 training loss: 53041422336.0
Epoch 165 training loss: 8213828352.0
Epoch 180 training loss: 1077095680.0
Epoch 195 training loss: 117650056.0
Epoch 210 training loss: 10439555.0
Epoch 225 training loss: 722636.375
Epoch 240 training loss: 36275.994140625
Epoch 255 training loss: 1126.4267578125
Epoch 270 training loss: 12.93126106262207
Epoch 285 training loss: 0.16127962619066238
Epoch 300 training loss: 0.06155513413250446
Epoch 315 training loss: 0.003355390625074506
Epoch 330 training loss: 2.8886365726066288e-05
Stopped training from convergence condition at epoch 336
Stopped training after max epochs